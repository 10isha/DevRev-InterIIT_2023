{"cells":[{"cell_type":"markdown","metadata":{"id":"2Bkd7w2YySGm"},"source":["# Finetuning script for DeLADE\n","\n","## Run instructions:\n","Only Change these variables in the Variables Section:\n","\n","1. **save_path**: Path where DeLADE checkpoints will be periodically saved during finetuning\n","\n","2. **data_url**: The download url of the train data csv (Please Refer to Generating the Download Link)\n","\n","4. **split**: The Percentage of Train, Validation and Test\n","\n","5. **DATA_DIR**: Path where training data is saved.\n","\n","6. Generating the Download Link:\n","  * Upload the Csv to Drive.\n","\n","  * Change the share settings and set view to all.\n","\n","  * Get the share link and generate the download link from https://sites.google.com/site/gdocs2direct/\n","\n","7. After setting the Appropriate Variables, Run all."]},{"cell_type":"markdown","metadata":{"id":"aJcgJ6OxSM6J"},"source":["# Variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yq2jIgw1SPrl"},"outputs":[],"source":["import os\n","# Get the Current Working Directory\n","cwd = os.getcwd()\n","\n","# The Path where the Finetuned Model checkpoints will be saved\n","save_path = cwd+\"/finetuned_model\"\n","\n","# The Path of the data files, data path contains a csv file in the Squadv2 format\n","data_url = 'https://drive.google.com/u/1/uc?id=1jHR-T1PH4xkd4ljGWn8lD-HaJPYMo4EC&export=download'\n","\n","# The Train - Validation - Split Percentage\n","split = [80,10,10] # 80% for Training, 10% for Validation and rest of the 10% for Testing\n","\n","# The Path where the training data for finetuning will be saved\n","DATA_DIR = cwd"]},{"cell_type":"markdown","metadata":{"id":"kHlFU2cg4Ayb"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hKaZ71vqT4M8"},"outputs":[],"source":["!git clone https://github.com/castorini/dhr.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PvgA-NSZV7nN"},"outputs":[],"source":["!pip install transformers datasets nmslib sentence-transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dxoa0FO7V0mg"},"outputs":[],"source":["%cd dhr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IF_wx913hz7s"},"outputs":[],"source":["!sudo apt install megatools"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsHv9TzXhvo9"},"outputs":[],"source":["!rm /content/dhr/tevatron/data.py\n","!megadl --path $cwd/dhr/tevatron/ 'https://mega.nz/#!8rpzDYyL!AuHeWGGAIbpAV4vBm91A8KNjzXjOvvsW1ON_pQGqoS0'\n","!rm /content/dhr/tevatron/trainer.py\n","!megadl --path $cwd/dhr/tevatron/ 'https://mega.nz/#!VjJwzbhR!_cy2OBYGUUQPVD_Zy4ez1iutSG5Gh6jSwPmPAcT5318'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cCEBezCZXla5"},"outputs":[],"source":["# Nmslib Params, used for generation of the train data for finetuning DeLADE\n","\n","M = 100\n","efC = 2000\n","\n","num_threads = 4\n","efS = 2000\n","query_time_params = {'efSearch': efS}\n","index_time_params = {'M': M, 'indexThreadQty': num_threads, 'efConstruction': efC, 'post' : 0}\n","nmslib_space = 'cosinesimil'\n","nmslib_method = 'hnsw'\n","\n","# Model card for the embedding model for NMSLib (for use as helper module)\n","embed_model_name = 'all-distilroberta-v1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pHrRutebjKjS"},"outputs":[],"source":["!pip install gdown\n","import gdown"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-X8C9_HDjL0C"},"outputs":[],"source":["# Download training files using gdown\n","data_path = f\"{DATA_DIR}/train_data.csv\"\n","gdown.download(data_url, data_path, quiet=False)"]},{"cell_type":"markdown","metadata":{"id":"F8DJ0LN64KJR"},"source":["# Preprocessing code for input data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZzc3lPn_oW5"},"outputs":[],"source":["import pandas as pd\n","DATA_PATH=cwd\n","df = pd.read_csv(data_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLrA9Ld-_oSc"},"outputs":[],"source":["paras = df.Paragraph.unique()\n","df[df[\"Paragraph\"]==paras[0]].Theme[0]\n","df = df.rename(columns={'Answer_Text':'Answer_text'})\n","data = []\n","for i, para in enumerate(paras):\n","    data_dict = {}\n","    data_dict['id']=i+1\n","    data_dict['paragraph']=para\n","    data_dict['theme'] = df[df[\"Paragraph\"]==para].iloc[0].Theme\n","    data.append(data_dict)\n","df2 = pd.DataFrame(data)\n","df2.to_csv(DATA_PATH+'/input_para1.csv', header=True, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_P-yyHzjwSOc"},"outputs":[],"source":["para_data_path = DATA_PATH+'/input_para1.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1e3-TZrxzRRD"},"outputs":[],"source":["# Functions for Loading and Saving the Data in the jsonl format (as required by\n","# DeLADE)\n","\n","def dump_jsonl(data, output_path, append=False):\n","    \"\"\"\n","    Write list of objects to a JSON lines file.\n","    \"\"\"\n","    mode = 'a+' if append else 'w'\n","    with open(output_path, mode, encoding='utf-8') as f:\n","        for line in data:\n","            json_record = json.dumps(line, ensure_ascii=False)\n","            f.write(json_record + '\\n')\n","    print('Wrote {} records to {}'.format(len(data), output_path))\n","def load_jsonl(input_path) -> list:\n","    \"\"\"\n","    Read list of objects from a JSON lines file.\n","    \"\"\"\n","    data = []\n","    with open(input_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            data.append(json.loads(line.rstrip('\\n|\\r')))\n","    print('Loaded {} records from {}'.format(len(data), input_path))\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M1SlxlKyRe74"},"outputs":[],"source":["# Loading Tokenizer for generating tokens for Training \n","\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\n","        'jacklin/DeLADE-CLS-P',\n","        # cache_dir=cache_dir,\n","        use_fast=False,\n","    )\n","\n","def preprocess_text(text,max_length=512):\n","  text_encoded = tokenizer.encode(\n","              tokenizer.sep_token.join(text),\n","              add_special_tokens=False,\n","              max_length=max_length,\n","              truncation=True\n","          )\n","  return text_encoded"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4aXWxA9tovKH"},"outputs":[],"source":["# Generating Paragraph Embeddings for NMSLIB search for generating Negative Para\n","# Ids, as required by DeLADE\n","\n","from sentence_transformers import SentenceTransformer\n","import pandas as pd\n","from tqdm import tqdm\n","embed_model = SentenceTransformer(embed_model_name)\n","para_df = pd.read_csv(para_data_path)\n","themes = para_df[\"theme\"].unique().tolist()\n","all_theme_embeddings = dict()\n","for theme in tqdm(themes):\n","  paras = para_df[para_df[\"theme\"]==theme].paragraph.unique()\n","  theme_para_embed = embed_model.encode(paras)\n","  all_theme_embeddings[theme] = theme_para_embed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8rentTXikdmj"},"outputs":[],"source":["# Generating Query Embeddings for NMSLIB search for generating Negative Para,\n","# Ids, as required by DeLADE\n","from ast import literal_eval\n","embed_model = SentenceTransformer(embed_model_name)\n","full_data = df.copy()\n","full_data.Answer_start = full_data.Answer_start.apply(literal_eval)\n","full_data.Answer_text = full_data.Answer_text.apply(literal_eval)\n","full_data['Unnamed: 0'] = full_data['Unnamed: 0'].astype(str)\n","queries = {}\n","for theme in tqdm(themes):\n","  theme_df = full_data[full_data['Theme']==theme]\n","  queries[theme] = {idx:embed_model.encode(theme_row['Question']) for idx,theme_row in theme_df.iterrows()}\n","# np.save(f'/content/drive/MyDrive/Assets/Embeddings/distilroberta_75k_queries.npy',queries,allow_pickle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yvCJsvljvSbX"},"outputs":[],"source":["# Splitting and Generating the Data\n","import numpy as np\n","import nmslib\n","train_samples = []\n","dev_samples = []\n","test_samples = []\n","\n","for theme in tqdm(themes):\n","  theme_df = full_data[full_data['Theme']==theme]\n","  n = len(theme_df)\n","  index = nmslib.init(method=nmslib_method, space=nmslib_space, data_type=nmslib.DataType.DENSE_VECTOR) \n","  index.addDataPointBatch(all_theme_embeddings[theme])\n","  index.createIndex(index_time_params) \n","  index.setQueryTimeParams(query_time_params)\n","  for i,theme_row in enumerate(theme_df.iterrows()):\n","    idx,theme_row = theme_row\n","    k=np.random.randint(5,10)\n","    I , D = index.knnQuery(queries[theme][idx], k = k)\n","    pred_para = theme_df.Paragraph.unique()[I].tolist()\n","    real_para = theme_row.Paragraph\n","    if real_para in pred_para:\n","      pred_para.remove(real_para)\n","    input = {\n","              'query_id':theme_row['Unnamed: 0'],\n","              'query':theme_row['Question'],\n","              'positive_passages':[{'doc_id':para_df[para_df.paragraph==real_para]['id'].tolist()[0],\n","                                   'title':theme,\n","                                   'text':real_para\n","                  }],\n","              'negative_passages':[\n","                  {\n","                   'doc_id':para_df[para_df.paragraph==para]['id'].tolist()[0],\n","                   'title':theme,\n","                   'text':para\n","                  } for para in pred_para],\n","          }\n","    if i<int(split[0]*n/sum(split)):\n","      train_samples.append(input)\n","    elif i<int((split[0]+split[1])*n/sum(split)):\n","      dev_samples.append(input)\n","    else:\n","      test_samples.append(input)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uomkTDcTQtUB"},"outputs":[],"source":["# Generating Tokens for the Data\n","\n","query_toks = {row['Unnamed: 0']: preprocess_text(row['Question']) for idx,row in full_data.iterrows()}\n","para_toks = {row['id']:preprocess_text(row['paragraph']) for idx,row in para_df.iterrows()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GiE5sOzGLdZ3"},"outputs":[],"source":["# Generating the Train Data\n","\n","train_data = []\n","for sample in tqdm(train_samples):\n","  train_data.append({\n","      'query': query_toks[sample['query_id']],\n","      'positive_pids': [sample['positive_passages'][0]['doc_id']],\n","      'negative_pids': [neg['doc_id'] for neg in sample['negative_passages']]\n","  })"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"blDNkW2oYeMB"},"outputs":[],"source":["# Generating the Corpus Data\n","\n","corpus_data = []\n","for id,tok in para_toks.items():\n","  corpus_data.append(\n","      {\n","          'text_id': str(id),\n","          'text': tok\n","      }\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1uqvAD7PHu3"},"outputs":[],"source":["# Saving the Generated Data\n","\n","import os\n","import json\n","try:\n","  os.mkdir(f'{DATA_DIR}/train')\n","  os.mkdir(f'{DATA_DIR}/corpus')\n","except Exception as e:\n","  pass\n","\n","dump_jsonl(train_data,f'{DATA_DIR}/train/75k_trainsplit.json')\n","dump_jsonl(corpus_data,f'{DATA_DIR}/corpus/corpus.json')"]},{"cell_type":"markdown","metadata":{"id":"x0arKLe84GIo"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zEYRvH-3TGqN"},"outputs":[],"source":["!export CUDA_VISIBLE_DEVICES=0\n","!export MODEL=DHR\n","!export CLSDIM=128\n","!export DLRDIM=768\n","!export MODEL_DIR=${MODEL}_CLS${CLSDIM}\n","\n","!python -m tevatron.driver.train \\\n","  --output_dir $save_path/DHR \\\n","  --train_dir $DATA_DIR/train \\\n","  --corpus_dir $DATA_DIR/corpus \\\n","  --model_name_or_path jacklin/DeLADE-CLS-P  \\\n","  --do_train \\\n","  --save_steps 5000 \\\n","  --per_device_train_batch_size 4 \\\n","  --learning_rate 7e-6 \\\n","  --q_max_len 32 \\\n","  --p_max_len 150 \\\n","  --num_train_epochs 1 \\\n","  --add_pooler \\\n","  --model DHR \\\n","  --projection_out_dim 128 \\\n","  --train_n_passages 8 \\\n","  --dataloader_num_workers 2 \\\n","  --seed 42 \\\n","  --overwrite_output_dir \\\n","  --combine_cls "]}],"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1hGq19b6eyxoRahiB9415t1bT1unvCi5N","timestamp":1675747440388},{"file_id":"1P4chGShT0x56UneBFzRwsey4h4dK5jA1","timestamp":1675527041731},{"file_id":"1kdLVkK_RL9mj0Z9JG3ziz7q9burRQ6nX","timestamp":1675095131494}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.10"},"vscode":{"interpreter":{"hash":"73d41fdac83fc891b88f8c6d970584d21b54ca61e1bc00d779e118ba98b55913"}}},"nbformat":4,"nbformat_minor":0}
