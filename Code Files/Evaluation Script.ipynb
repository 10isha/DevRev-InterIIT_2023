{"cells":[{"cell_type":"markdown","metadata":{"id":"a87hy3aOrP6t"},"source":["# Instructions:\n","## Provide the path to the paragraph csv and question_answer csv under the variables para_data and question_ans.\n","## A finalprocessed.csv will be generated using which the required four csv's for evaluation will be generated in Final_data folder.\n","## Run all the cells as it is.\n","## The result will be saved at`{result_path}.csv`\n","## Scores will be printed in the last cell of evaluation section.\n"]},{"cell_type":"markdown","source":["## File Generation and Preprocessing"],"metadata":{"id":"58jqn49KVei7"}},{"cell_type":"code","source":["import os\n","import numpy as np"],"metadata":{"id":"LWuajsLM4XZM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","The 'para_data' and 'question_ans' variables take the files provided in the \n","first stage of evaluation, i.e.,\n","'''\n","cwd=os.getcwd()\n","\n","import pandas as pd\n","# Paragraph mapping file\n","para_data = pd.read_csv('https://drive.google.com/uc?export=download&id=1GNHhH81J1pEZSB-OSGRTtqIUQCCNhJ6I')\n","# Questions mapping file\n","question_ans = pd.read_csv('https://drive.google.com/uc?export=download&id=1GQ3-E7k60K16f47A18ptQfKUBkhxQudM')"],"metadata":{"id":"kuV_Feps_Szu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = question_ans\n","df2 = para_data"],"metadata":{"id":"D8qF9Ioz61Ec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["para_id_list = df.paragraph_id.tolist()\n","paras = []\n","for i in para_id_list:\n","    paras.append(df2['paragraph'][df2.id==i].tolist()[0])\n","df['paragraph'] = paras\n","df.rename(columns = {'question':'Question', 'theme':'Theme','answer':'Answer_text', 'paragraph':'Paragraph'}, inplace = True)\n","poss = []\n","for i in range(len(df)):\n","    poss.append('TRUE')\n","df['Answer_possible'] = poss\n","start = []\n","for i in range(len(df)):\n","  # print(df['Answer_text'][i])\n","  s = str(df['Answer_text'][i])\n","  start.append([df['Paragraph'][i].find(s)])\n","df['Answer_start'] = start\n","answers = df['Answer_text'].tolist()\n","ans = []\n","for i in answers:\n","    ans.append([i])\n","df['Answer_text'] = ans\n","df.to_csv(cwd+'/finaldataprocessed.csv')"],"metadata":{"id":"hyv8fUPX7PoM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["csv_save_path=cwd+'/finaldataprocessed.csv'"],"metadata":{"id":"ZdTWYqhJJw7k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(csv_save_path)"],"metadata":{"id":"bk2443j88Syv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.rename(columns={'Unnamed: 0':'id'})\n","df['id'] =df['id']+1\n","df.to_csv(cwd+'/finaldataprocessed.csv')"],"metadata":{"id":"TXtIEkukB65U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.mkdir(cwd+'/Final_data')"],"metadata":{"id":"IfdA7xYtCtc8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m=cwd+'/Final_data'\n","df = pd.read_csv(cwd+'/finaldataprocessed.csv')"],"metadata":{"id":"yZzc3lPn_oW5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["paras = df.Paragraph.unique()"],"metadata":{"id":"3StDoV14CrOe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation Files(4 CSVs) Generation"],"metadata":{"id":"NGmNU3FQvD8_"}},{"cell_type":"code","source":["val_folder=cwd+'/Final_data'\n","val_folder"],"metadata":{"id":"C0zmsfumMW32"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","if not os.path.exists(val_folder): os.makedirs(val_folder)"],"metadata":{"id":"Ef9ZYi4ov6ZM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TjrJ6vCVLUoE"},"outputs":[],"source":["df = pd.read_csv('finaldataprocessed.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCQQWsnaLUoH"},"outputs":[],"source":["paras = df.Paragraph.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mVv3glo4LUoI"},"outputs":[],"source":["df[df[\"Paragraph\"]==paras[0]].Theme[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7s51grJyLUoI"},"outputs":[],"source":["data = []\n","for i, para in enumerate(paras):\n","    data_dict = {}\n","    data_dict['id']=i+1\n","    data_dict['paragraph']=para\n","    data_dict['theme'] = df[df[\"Paragraph\"]==para].iloc[0].Theme\n","    data.append(data_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tCIyBnkHLUoK"},"outputs":[],"source":["df2 = pd.DataFrame(data)"]},{"cell_type":"code","source":["df_theme=df2.copy()"],"metadata":{"id":"vXGdtnSBkDQN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tWS-YPrLUoL"},"outputs":[],"source":["input_para=val_folder+'/input_para.csv'\n","df2.to_csv(input_para, header=True, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qg4FWtBDLUoM"},"outputs":[],"source":["data = []\n","for i in range(len(df.Question)):\n","    data_dict = dict()\n","    data_dict['id'] = i+1\n","    data_dict['question'] = df.Question[i]\n","    data_dict['theme'] = df.Theme[i]\n","    data.append(data_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"41Y6MDfALUoR"},"outputs":[],"source":["df2 = pd.DataFrame(data)\n","input_question=val_folder+'/input_question.csv'\n","df2.to_csv(input_question, header=True, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-_YsKrZsLUob"},"outputs":[],"source":["data = []\n","for i in range(len(df.Question)):\n","    data_dict = dict()\n","    data_dict['question_id'] = i+1\n","    if(df.Answer_start[i]=='[]'):\n","     data_dict['paragraph_id'] = -1\n","    else:\n","     data_dict['paragraph_id']=df_theme[df_theme.paragraph==df.Paragraph[i]].index.tolist()[0]+1\n","    data_dict['answers'] = df.Answer_text[i]\n","    data.append(data_dict)\n","df2 = pd.DataFrame(data)\n","ground_truth=val_folder+'/ground_truth.csv'\n","df2.to_csv(ground_truth, header=True, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCXQ4jPdLUod"},"outputs":[],"source":["data = []\n","for i in df.Theme.unique():\n","    data_dict = dict()\n","    start, end = (df.index[df['Theme']==i][[0,-1]]+1).tolist()\n","    data_dict[\"theme\"] = i\n","    data_dict[\"start\"] = start\n","    data_dict['end'] = end\n","    data.append(data_dict)\n","df2 = pd.DataFrame(data)\n","theme_interval=val_folder+'/theme_interval.csv'\n","df2.to_csv(theme_interval, header=True, index=False)"]},{"cell_type":"code","source":["print(input_para+'\\n'+input_question+'\\n'+ground_truth+'\\n'+theme_interval+'\\n')"],"metadata":{"id":"GmsQ25G9aWlz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Variables"],"metadata":{"id":"dpFVXKjcHTcm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ymVseocEKz_"},"outputs":[],"source":["\n","\n","#Path to the folder containing evaluation dataset(4 csv's)\n","test_data_path=cwd+'/Final_data' \n","\n","#Path to store result file\n","result_path=cwd+'/result.csv'\n","\n","#Name to store themewise file and delade predicted para file\n","m='pred_para'\n","\n","#Provide paths to the evaluation files\n","theme_path = cwd+'/Final_data/theme_interval.csv'\n","ques_data_path = cwd+'/Final_data/input_question.csv'\n","para_data_path =cwd+'/Final_data/input_para.csv'\n","\n","#Path to ground truth file\n","ground_truth_path = cwd+'/Final_data/ground_truth.csv'"]},{"cell_type":"markdown","metadata":{"id":"msOXj9lNMZJV"},"source":["# Imports"]},{"cell_type":"code","source":["!sudo apt install megatools"],"metadata":{"id":"xAtmpIpHItJ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1XzciHtqbef1"},"outputs":[],"source":["#Variables not to be changed\n","seed = 42\n","threshold=0.1\n","similarity_method='dot'"]},{"cell_type":"code","source":["%cd $cwd"],"metadata":{"id":"3giUDUUL7a99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/castorini/dhr.git"],"metadata":{"id":"BCEQs4oU62Pj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd dhr\n","!git clone https://huggingface.co/jacklin/DeLADE-CLS-P"],"metadata":{"id":"WzTB6WkY_MpR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!megadl --path $cwd/dhr/tevatron/datasets/beir/ 'https://mega.nz/#!luAz2IgI!q0R7urmIfhiTLhRpiBqs18Vpuriw6fVnQk_2ZkBPiBU'\n","!megadl --path $cwd/dhr/tevatron/datasets/beir/ 'https://mega.nz/#!YmYBQZha!TjnxWDN0V_vp7-c3pxPLPmFPWW60UGy-yPuaaxrheD4'\n","!megadl --path $cwd/dhr/tevatron/datasets/beir/ 'https://mega.nz/#!wz4VQYqR!oYV5DTqXhJ9BFwTeJE_BbeTnTlWQ8_JijUhgUhse9WE'\n","!megadl --path $cwd/dhr/tevatron/datasets/beir/ 'https://mega.nz/#!8uIBhbgS!RUmCXrIYNfFeUahldqQ6jL2duHuTozJA6q_HpWi_SBU'"],"metadata":{"id":"9dl298gO61bK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TFUijIS4H3A6"},"outputs":[],"source":["# Allowed to make changes.\n","!pip install sentence-transformers\n","!pip install beir\n","!pip install datasets\n","!sudo apt install megatools\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","import collections\n","import gdown\n","import json\n","import pandas as pd\n","import re\n","import string\n","import timeit\n","from ast import literal_eval\n","import pandas as pd\n","from google.colab import drive\n","from sentence_transformers import SentenceTransformer,util\n","from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n","import time\n","import torch.nn.utils.prune as prune\n","import numpy as np\n","import os\n","from transformers import pipeline,set_seed\n","import torch\n","torch.manual_seed(42)\n","set_seed(42)\n"]},{"cell_type":"markdown","metadata":{"id":"c1w0olVIQfVv"},"source":["# Conversion to beir format"]},{"cell_type":"markdown","metadata":{"id":"n77Ou9JyXqbp"},"source":["The evaluation data will be converted into beir format data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4IoK8YtXXPHF"},"outputs":[],"source":["#Change the path to store beir format of evaluation data,if required\n","beir_data_path=cwd+'/beir_format' "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B_tmlpCKQ9rC"},"outputs":[],"source":["import os\n","if not os.path.exists(beir_data_path): os.makedirs(beir_data_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wRTr3JoRBD2"},"outputs":[],"source":["para_df=pd.read_csv(para_data_path)\n","ques_df=pd.read_csv(ques_data_path)\n","theme_df=pd.read_csv(theme_path)\n","ground_df=pd.read_csv(ground_truth_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-iyUuETLRHLN"},"outputs":[],"source":["para_df.rename(columns={'theme':'title'},inplace=True)\n","para_df.rename(columns={'paragraph':'text'},inplace=True)\n","para_df.rename(columns={'id':'_id'},inplace=True)\n","para_df['_id'] = para_df['_id'].apply(lambda x: str(x))\n","para_df.to_csv(f'{beir_data_path}/corpus.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VAwEbY1NRPnf"},"outputs":[],"source":["ques_df.rename(columns={'theme':'title'},inplace=True)\n","ques_df.rename(columns={'question':'text'},inplace=True)\n","ques_df.rename(columns={'id':'_id'},inplace=True)\n","ques_df['_id'] = ques_df['_id'].apply(lambda x: str(x))"]},{"cell_type":"code","source":["ground_df.pop('answers')"],"metadata":{"id":"I4fIhjTLQdjG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6msvy1pRRS2k"},"outputs":[],"source":["\n","ground_df['question_id'] = ground_df['question_id'].apply(lambda x: str(x))\n","ground_df['paragraph_id'] = ground_df['paragraph_id'].apply(lambda x: str(x))\n","ground_df.rename(columns={'paragraph_id':'corpus-id'},inplace=True)\n","ground_df.rename(columns={'question_id':'query-id'},inplace=True)\n","ground_df['score']=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"liDwnbRBRV_3"},"outputs":[],"source":["import json\n","import os\n","themes=ques_df['title'].unique()\n","for theme in themes:\n","  os.mkdir(f'{beir_data_path}/{theme}')\n","  os.mkdir(f'{beir_data_path}/{theme}/qrels')\n","  corpus_df = para_df[para_df[\"title\"]==theme]\n","  corpus_df.to_json(f'{beir_data_path}/{theme}/corpus.jsonl',orient='records',lines=True)\n","  queries_df=ques_df[ques_df[\"title\"]==theme]\n","  queries_df.to_json(f'{beir_data_path}/{theme}/queries.jsonl',orient='records',lines=True)\n","  start = theme_df[theme_df['theme']==theme]['start'].tolist()[0]\n","  end = theme_df[theme_df['theme']==theme]['end'].tolist()[0]\n","  qrels=ground_df.iloc[start-1:end]\n","  qrels.to_csv(f'{beir_data_path}/{theme}/qrels/test.tsv',index=False,sep='\\t')"]},{"cell_type":"markdown","metadata":{"id":"iboDsguP61Ov"},"source":["# Delade generate embeddings\n"]},{"cell_type":"markdown","metadata":{"id":"hux_zZL3TXd1"},"source":["Run this cell to generate embeddings.\n","\n","\n","Embeddings will be saved at `/new_delade_embeddings_corpus.npy`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5L4sZZ0E64cT"},"outputs":[],"source":["%cd cwd+'/dhr'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-DFkzK808FDU"},"outputs":[],"source":["emb_delade_dhr_path=cwd+'/dhr'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G36dcq40N8d6"},"outputs":[],"source":["!pip install torch>=1.7.0\n","# !pip install transformers==4.15.0\n","!pip install pyserini\n","!pip install beir\n","!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJp4r-SYOBbz"},"outputs":[],"source":["import pandas as pd\n","para_df=pd.read_csv(f'{beir_data_path}/corpus.csv')"]},{"cell_type":"code","source":["!$cwd/dhr/DeLADE-CLS-P"],"metadata":{"id":"v890zaPMmUt1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HXEnDFfmOEH2"},"outputs":[],"source":["#Will be generated in /cwd/new_delade_embeddings_corpus.npy\n","# Ignore the error. Some changes have been made for reducing the embedding generation time\n","themes=para_df['title'].unique()\n","for theme in themes:\n","  !python -m tevatron.datasets.beir.encode_and_retrieval_emb --theme f'{theme}' --model_name_or_path {cwd}/dhr/DeLADE-CLS-P --dataPath f'{beir_data_path}'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k5s9DNz-adoU"},"outputs":[],"source":["embeddings_path=cwd+'/new_delade_embeddings_corpus.npy'"]},{"cell_type":"markdown","metadata":{"id":"_CRnwN4e-WF1"},"source":["# Delade load embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6holUyH8-r2H"},"outputs":[],"source":["import numpy as np\n","ab=np.load(embeddings_path,allow_pickle=True)\n","all_theme_embeddings=ab.item()"]},{"cell_type":"markdown","metadata":{"id":"534t-fV22mGo"},"source":["# Load QA base "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0tM8h2Hk2q03"},"outputs":[],"source":["%cd $cwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DJ-e6o8v22RT"},"outputs":[],"source":["nlp=None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uqFq1X5z25Fm"},"outputs":[],"source":["model_name = \"mrm8488/electra-small-finetuned-squadv2\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x8Pnsq3h27JF"},"outputs":[],"source":["nlp = pipeline(\"question-answering\", model = model_name)"]},{"cell_type":"markdown","metadata":{"id":"3fjx_8xEuAM3"},"source":["# Load Cross Encoder"]},{"cell_type":"code","source":["%cd $cwd"],"metadata":{"id":"0nceJXWO98U1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!megadl --path {cwd}/ 'https://mega.nz/#!8zoUjZKL!iSOdg7ySmNut-mUaRDI0nO5uMZRJLg9R59DxLP3aQvU'"],"metadata":{"id":"oqdCpJ3v9-t-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip {cwd}/minilml4-v2_17-20230205T094107Z-001.zip"],"metadata":{"id":"bSSvzrW7-X96"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mEPGqTObAdcM"},"outputs":[],"source":["from sentence_transformers import CrossEncoder\n","cross_enc_ft_model=cwd+'/minilml4-v2_17'\n","cross_model = CrossEncoder(cross_enc_ft_model,max_length=512)\n"]},{"cell_type":"markdown","metadata":{"id":"Lb-Djjy5OKSR"},"source":["# Evaluation at k=3\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDFTtfTTUfm_"},"outputs":[],"source":[" %cd $cwd/dhr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BQxJ8ngAOMhs"},"outputs":[],"source":["# Allowed to make changes.\n","def get_theme_model(theme):\n","  global_model = nlp\n","  return global_model\n","def Average(lis):\n","  return sum(lis)/len(lis)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQ1EmUviOROD"},"outputs":[],"source":["\n","para_df2=pd.read_csv(f'{beir_data_path}/corpus.csv')\n","para_df=pd.read_csv(para_data_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IyXy-6SeOThM"},"outputs":[],"source":["# Allowed to make changes.\n","#for direct delade(k=5)\n","def pred_theme_ans(questions, theme_model, pred_out):\n","  theme = questions[0]['theme']\n","\n","  !python -m tevatron.datasets.beir.encode_and_retrieval_new --theme f'{theme}' --model_name_or_path {cwd}/dhr/DeLADE-CLS-P --m f'{m}_res.csv' --sim f'{similarity_method}' --data_path f'{beir_data_path}' \n","\n","  for question in questions:\n","    ans = {}\n","    ans['question_id'] = question['id']    \n","    res5=pd.read_csv(cwd+f'/{m}_res.csv')\n","    l=len(para_df2.loc[para_df2['title']==theme].text.unique())\n","    \n","    if(l>=3):\n","      pred_para=[para_df2.loc[para_df2['_id']==((res5.loc[res5['id']==question['id']].C.unique()[0]))].text.unique()[0],para_df2.loc[para_df2['_id']==((res5.loc[res5['id']==question['id']].B.unique()[0]))].text.unique()[0],para_df2.loc[para_df2['_id']==((res5.loc[res5['id']==question['id']].A.unique()[0]))].text.unique()[0]]    \n","    elif(l==2):\n","      pred_para=[para_df2.loc[para_df2['_id']==((res5.loc[res5['id']==question['id']].B.unique()[0]))].text.unique()[0],para_df2.loc[para_df2['_id']==((res5.loc[res5['id']==question['id']].A.unique()[0]))].text.unique()[0]]   \n","    elif(l==1):\n","      pred_para=[para_df2.loc[para_df2['_id']==((res5.loc[res5['id']==question['id']].A.unique()[0]))].text.unique()[0]]\n","    scores = cross_model.predict([(question['question'],p) for p in pred_para])\n","    best = np.argmax(scores)\n","    result = theme_model(question=question['question'], context=pred_para[best])\n","    answer = result['answer']\n","    if result['score'] < threshold:\n","      ans['paragraph_id'] = -1\n","      ans['answers'] = ''\n","    else:\n","      ans['answers'] = answer\n","      ans['paragraph_id'] = para_df[para_df['paragraph']==pred_para[best]]['id'].tolist()[0]\n","    pred_out.append(ans)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RBHd81IeOVRQ"},"outputs":[],"source":["# NOT allowed to make changes. \n","from tqdm import tqdm\n","# All theme prediction.\n","questions = json.loads(pd.read_csv(ques_data_path).to_json(orient=\"records\"))\n","theme_intervals = json.loads(pd.read_csv(theme_path).to_json(orient=\"records\"))\n","pred_out = []\n","theme_inf_time = {}\n","execution_times = []\n","res=pd.DataFrame()\n","res['id']=None\n","res['A']=None\n","res['B']=None\n","res['C']=None\n","res['D']=None\n","res['E']=None\n","res.to_csv(cwd+f'/{m}_res.csv',index=False)\n","for theme_interval in tqdm(theme_intervals):\n","  theme_ques = questions[int(theme_interval[\"start\"]) - 1: int(theme_interval[\"end\"])]\n","  theme = theme_ques[0][\"theme\"]\n","  # Load model fine-tuned for this theme.\n","  theme_model = get_theme_model(theme)\n","  execution_time = timeit.timeit(lambda: pred_theme_ans(theme_ques, theme_model, pred_out), number=1)\n","  execution_times.append(execution_time)\n","  theme_inf_time[theme_interval[\"theme\"]] = execution_time * 1000 # in milliseconds.\n","pred_df = pd.DataFrame.from_records(pred_out)\n","# Write prediction to a CSV file. Teams are required to submit this csv file.\n","pred_df.to_csv(result_path, index=False)\n","theme_inf_df = pd.DataFrame(list(theme_inf_time.items()),columns = ['theme','avg_inf_time']) \n","theme_inf_df.to_csv(cwd+f'/{m}_inf_time.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eG16pww3OXhj"},"outputs":[],"source":["lst=theme_inf_df['avg_inf_time']\n","theme_int=pd.read_csv(theme_path)\n","theme_times = [ex_time/(theme_int['end'][i]-theme_int['start'][i]+1) for i,ex_time in enumerate(lst)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJqRAFBHOgPL"},"outputs":[],"source":["\n","def normalize_answer(s):\n","  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","  def remove_articles(text):\n","    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n","    return re.sub(regex, ' ', text)\n","  def white_space_fix(text):\n","    return ' '.join(text.split())\n","  def remove_punc(text):\n","    exclude = set(string.punctuation)\n","    return ''.join(ch for ch in text if ch not in exclude)\n","  def lower(text):\n","    return text.lower()\n","  return white_space_fix(remove_articles(remove_punc(lower(str(s)))))\n","\n","def get_tokens(s):\n","  if not s: return []\n","  return normalize_answer(s).split()\n","\n","def calc_f1(a_gold, a_pred):\n","  gold_toks = get_tokens(a_gold)\n","  pred_toks = get_tokens(a_pred)\n","  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n","  num_same = sum(common.values())\n","  if len(gold_toks) == 0 or len(pred_toks) == 0:\n","    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n","    return int(gold_toks == pred_toks)\n","  if num_same == 0:\n","    return 0\n","  precision = 1.0 * num_same / len(pred_toks)\n","  recall = 1.0 * num_same / len(gold_toks)\n","  f1 = (2 * precision * recall) / (precision + recall)\n","  return f1\n","\n","def calc_max_f1(predicted, ground_truths):\n","  max_f1 = 0\n","  for ground_truth in ground_truths:\n","    f1 = calc_f1(predicted, ground_truth)\n","    max_f1 = max(max_f1, f1)\n","  try:\n","    ground_truths[0]\n","  except Exception as e:\n","    if predicted!=predicted:\n","      max_f1 = 1\n","  return max_f1\n","\n","def normalize_answer(s):\n","  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","  def remove_articles(text):\n","    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n","    return re.sub(regex, ' ', text)\n","  def white_space_fix(text):\n","    return ' '.join(text.split())\n","  def remove_punc(text):\n","    exclude = set(string.punctuation)\n","    return ''.join(ch for ch in text if ch not in exclude)\n","  def lower(text):\n","    return text.lower()\n","  return white_space_fix(remove_articles(remove_punc(lower(str(s)))))\n","\n","def get_tokens(s):\n","  if not s: return []\n","  return normalize_answer(s).split()\n","\n","def calc_f1(a_gold, a_pred):\n","  gold_toks = get_tokens(a_gold)\n","  pred_toks = get_tokens(a_pred)\n","  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n","  num_same = sum(common.values())\n","  if len(gold_toks) == 0 or len(pred_toks) == 0:\n","    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n","    return int(gold_toks == pred_toks)\n","  if num_same == 0:\n","    return 0\n","  precision = 1.0 * num_same / len(pred_toks)\n","  recall = 1.0 * num_same / len(gold_toks)\n","  f1 = (2 * precision * recall) / (precision + recall)\n","  return f1\n","\n","def calc_max_f1(predicted, ground_truths):\n","  max_f1 = 0\n","  for ground_truth in ground_truths:\n","    f1 = calc_f1(predicted, ground_truth)\n","    max_f1 = max(max_f1, f1)\n","  try:\n","    ground_truths[0]\n","  except Exception as e:\n","    if predicted!=predicted:\n","      max_f1 = 1\n","  return max_f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LCOXykrgOknM"},"outputs":[],"source":["# NOT allowed to make changes. \n","\n","# Evaluation methodology.\n","metrics = {}\n","total_f1=0\n","total_f2=0\n","pred = pd.read_csv(result_path)\n","truth = pd.read_csv(ground_truth_path)\n","truth.paragraph_id = truth.paragraph_id\n","# truth.paragraph_id = truth.paragraph_id.apply(literal_eval)\n","# truth.answers = truth.answers.apply(literal_eval)\n","questions = pd.read_csv(ques_data_path)\n","for idx in pred.index:\n","  q_id = pred[\"question_id\"][idx]\n","  q_rows = questions.loc[questions['id'] == q_id].iloc[-1]\n","  theme = q_rows[\"theme\"]\n","  predicted_paragraph = pred[\"paragraph_id\"][idx]\n","  predicted_ans = pred[\"answers\"][idx]\n","  \n","  if theme not in metrics.keys():\n","    metrics[theme] = {\"true_positive\": 0, \"true_negative\": 0, \"total_predictions\": 0, \"f1_sum\": 0}\n","\n","  truth_row = truth.loc[truth['question_id'] == q_id].iloc[-1]\n","  truth_paragraph_id = truth_row[\"paragraph_id\"]\n","  # -1 prediction in case there is no paragraph which can answer the query.\n","  if predicted_paragraph == -1 and truth_row[\"paragraph_id\"] == -1:\n","    # Increase TN.\n","    metrics[theme][\"true_negative\"] = metrics[theme][\"true_negative\"] + 1\n","    total_f2+=1\n","  elif predicted_paragraph == truth_paragraph_id:\n","    # Increase TP for that theme.\n","    metrics[theme][\"true_positive\"] = metrics[theme][\"true_positive\"] + 1\n","    total_f2+=1\n","  # Increase total predictions for that theme.\n","  metrics[theme][\"total_predictions\"] = metrics[theme][\"total_predictions\"] + 1\n","  f1 = calc_f1(predicted_ans, truth_row[\"answers\"])\n","  metrics[theme][\"f1_sum\"] = metrics[theme][\"f1_sum\"] + f1\n","  total_f1+=f1\n","final_f1 = round(total_f1/len(questions),3)\n","final_f2 = round(total_f2/len(questions),3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BAy-IHmzOpM4"},"outputs":[],"source":["# NOT allowed to make changes.\n","theme_inf_df = pd.read_csv(cwd+f'/{m}_inf_time.csv')\n","theme_inf_time = {theme:theme_inf_df[theme_inf_df['theme']==theme]['avg_inf_time'].tolist()[0] for theme in metrics}\n","no_of_themes=len(theme_inf_df)\n","# Final score.\n","inf_time_threshold = 1000.0  # milliseconds.\n","final_para_score = 0.0\n","final_qa_score = 0.0\n","\n","for theme in metrics:\n","  inf_time_score = 1.0\n","  metric = metrics[theme]\n","  para_score = (metric[\"true_positive\"] + metric[\"true_negative\"]) / metric[\"total_predictions\"] \n","  qa_score = metric[\"f1_sum\"] / metric[\"total_predictions\"]\n","  avg_inf_time = theme_inf_time[theme] / metric[\"total_predictions\"]\n","  if avg_inf_time > inf_time_threshold:\n","    inf_time_score = inf_time_threshold / avg_inf_time\n","  final_qa_score += 1/(no_of_themes) * inf_time_score * qa_score\n","  final_para_score += 1/(no_of_themes) * inf_time_score * para_score\n","\n","print('f1_score:',final_f1*100)\n","print('para_score:',round(final_para_score,3)*100)\n","print('qa_score:',round(final_qa_score,3)*100)\n","print('average_inference_time:',round(Average(theme_times),0))\n","print('median time:',round(np.sort(theme_times)[len(theme_times)//2],0))\n"]},{"cell_type":"markdown","metadata":{"id":"TnvC6W9VNO68"},"source":["# Evaluation at k=3(with caching)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bm9yywp7NO69"},"outputs":[],"source":[" %cd $cwd/dhr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdi4uI5WNO69"},"outputs":[],"source":["# Allowed to make changes.\n","def get_theme_model(theme):\n","  global_model = nlp\n","  return global_model\n","def Average(lis):\n","  return sum(lis)/len(lis)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"byHuH6RFNO6-"},"outputs":[],"source":["\n","para_df2=pd.read_csv(f'{beir_data_path}/corpus.csv')\n","para_df=pd.read_csv(para_data_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XEH43bg5NO6-"},"outputs":[],"source":["num_ques_cache = 0 # number of question cached\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","# evaluation function\n","def pred_theme_ans(questions, theme_model, pred_out):\n","\n","  theme = questions[0]['theme']\n","\n","\n","  !python -m tevatron.datasets.beir.encode_and_retrieval_new --theme f'{theme}' --model_name_or_path {cwd}/dhr/DeLADE-CLS-P --m f'{m}_res.csv' --sim f'{similarity_method}' --data_path f'{beir_data_path}' \n","\n","  \n","  # a file 'delade_embeddings_queries.npy' containing embeddings of all the questions in the theme is created\n","  query_embeds = np.load(cwd+'/delade_embeddings_queries.npy',allow_pickle=True).item() # loading embeddings of all the queries in the theme\n","  cos_scores = cosine_similarity(query_embeds['queries']) # computing cosine similarity scores of all the possible pairs of queries in 'query_embeds'\n","  cached_paras = []\n","\n","  for k,question in enumerate(questions):\n","\n","    ans = {}\n","    ans['question_id'] = question['id'] \n","    final_para = 'blank'\n","\n","    if k>0:\n","      if np.max(cos_scores[k][:k]) > 0.8: \n","        final_para = cached_paras[np.argmax(cos_scores[k][:k])] # use the cached para if similarity score is greater than 0.8\n","        result = theme_model(question=question['question'], context=final_para)\n","        cached_paras.append(final_para)  \n","        global num_ques_cache\n","        num_ques_cache += 1\n","\n","    if final_para == 'blank':\n","      res5=pd.read_csv(cwd+f'/{m}_res.csv')\n","      l=len(para_df2.loc[para_df2['title']==theme].text.unique())\n","      if(l>=3):\n","        pred_para=[para_df2.loc[para_df2['_id']==((res5.loc[res5['id']==question['id']].C.unique()[0]))].text.unique()[0],para_df2.loc[para_df2['_id']==((res5.loc[res5['id']==question['id']].B.unique()[0]))].text.unique()[0],para_df2.loc[para_df2['_id']==((res5.loc[res5['id']==question['id']].A.unique()[0]))].text.unique()[0]]    \n","      elif(l==2):\n","        pred_para=[para_df2.loc[para_df2['_id']==((res5.loc[res5['id']==question['id']].B.unique()[0]))].text.unique()[0],para_df2.loc[para_df2['_id']==((res5.loc[res5['id']==question['id']].A.unique()[0]))].text.unique()[0]]   \n","      elif(l==1):\n","        pred_para=[para_df2.loc[para_df2['_id']==((res5.loc[res5['id']==question['id']].A.unique()[0]))].text.unique()[0]]\n","      scores = cross_model.predict([(question['question'],p) for p in pred_para])\n","      final_para = pred_para[np.argmax(scores)]\n","      result = theme_model(question=question['question'], context=final_para)\n","      cached_paras.append(final_para)\n","\n","    answer = result['answer']\n","    if result['score'] < threshold:\n","      ans['paragraph_id'] = -1\n","      ans['answers'] = ''\n","    else:\n","      ans['answers'] = answer\n","      ans['paragraph_id'] = 1 + para_df.index[para_df['paragraph']==final_para].tolist()[0]\n","      \n","    pred_out.append(ans)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sufa_1j1NO6-"},"outputs":[],"source":["# NOT allowed to make changes. \n","from tqdm import tqdm\n","# All theme prediction.\n","questions = json.loads(pd.read_csv(ques_data_path).to_json(orient=\"records\"))\n","theme_intervals = json.loads(pd.read_csv(theme_path).to_json(orient=\"records\"))\n","pred_out = []\n","theme_inf_time = {}\n","execution_times = []\n","res=pd.DataFrame()\n","res['id']=None\n","res['A']=None\n","res['B']=None\n","res['C']=None\n","res['D']=None\n","res['E']=None\n","res.to_csv(cwd+f'/{m}_res.csv',index=False)\n","for theme_interval in tqdm(theme_intervals):\n","  theme_ques = questions[int(theme_interval[\"start\"]) - 1: int(theme_interval[\"end\"])]\n","  theme = theme_ques[0][\"theme\"]\n","  # Load model fine-tuned for this theme.\n","  theme_model = get_theme_model(theme)\n","  execution_time = timeit.timeit(lambda: pred_theme_ans(theme_ques, theme_model, pred_out), number=1)\n","  execution_times.append(execution_time)\n","  theme_inf_time[theme_interval[\"theme\"]] = execution_time * 1000 # in milliseconds.\n","pred_df = pd.DataFrame.from_records(pred_out)\n","# Write prediction to a CSV file. Teams are required to submit this csv file.\n","pred_df.to_csv(result_path, index=False)\n","theme_inf_df = pd.DataFrame(list(theme_inf_time.items()),columns = ['theme','avg_inf_time']) \n","theme_inf_df.to_csv(cwd+f'/{m}_inf_time.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4q0zHkf5NO6-"},"outputs":[],"source":["lst=theme_inf_df['avg_inf_time']\n","theme_int=pd.read_csv(theme_path)\n","theme_times = [ex_time/(theme_int['end'][i]-theme_int['start'][i]+1) for i,ex_time in enumerate(lst)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VlsVcFSBNO6-"},"outputs":[],"source":["\n","def normalize_answer(s):\n","  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","  def remove_articles(text):\n","    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n","    return re.sub(regex, ' ', text)\n","  def white_space_fix(text):\n","    return ' '.join(text.split())\n","  def remove_punc(text):\n","    exclude = set(string.punctuation)\n","    return ''.join(ch for ch in text if ch not in exclude)\n","  def lower(text):\n","    return text.lower()\n","  return white_space_fix(remove_articles(remove_punc(lower(str(s)))))\n","\n","def get_tokens(s):\n","  if not s: return []\n","  return normalize_answer(s).split()\n","\n","def calc_f1(a_gold, a_pred):\n","  gold_toks = get_tokens(a_gold)\n","  pred_toks = get_tokens(a_pred)\n","  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n","  num_same = sum(common.values())\n","  if len(gold_toks) == 0 or len(pred_toks) == 0:\n","    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n","    return int(gold_toks == pred_toks)\n","  if num_same == 0:\n","    return 0\n","  precision = 1.0 * num_same / len(pred_toks)\n","  recall = 1.0 * num_same / len(gold_toks)\n","  f1 = (2 * precision * recall) / (precision + recall)\n","  return f1\n","\n","def calc_max_f1(predicted, ground_truths):\n","  max_f1 = 0\n","  for ground_truth in ground_truths:\n","    f1 = calc_f1(predicted, ground_truth)\n","    max_f1 = max(max_f1, f1)\n","  try:\n","    ground_truths[0]\n","  except Exception as e:\n","    if predicted!=predicted:\n","      max_f1 = 1\n","  return max_f1\n","\n","def normalize_answer(s):\n","  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","  def remove_articles(text):\n","    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n","    return re.sub(regex, ' ', text)\n","  def white_space_fix(text):\n","    return ' '.join(text.split())\n","  def remove_punc(text):\n","    exclude = set(string.punctuation)\n","    return ''.join(ch for ch in text if ch not in exclude)\n","  def lower(text):\n","    return text.lower()\n","  return white_space_fix(remove_articles(remove_punc(lower(str(s)))))\n","\n","def get_tokens(s):\n","  if not s: return []\n","  return normalize_answer(s).split()\n","\n","def calc_f1(a_gold, a_pred):\n","  gold_toks = get_tokens(a_gold)\n","  pred_toks = get_tokens(a_pred)\n","  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n","  num_same = sum(common.values())\n","  if len(gold_toks) == 0 or len(pred_toks) == 0:\n","    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n","    return int(gold_toks == pred_toks)\n","  if num_same == 0:\n","    return 0\n","  precision = 1.0 * num_same / len(pred_toks)\n","  recall = 1.0 * num_same / len(gold_toks)\n","  f1 = (2 * precision * recall) / (precision + recall)\n","  return f1\n","\n","def calc_max_f1(predicted, ground_truths):\n","  max_f1 = 0\n","  for ground_truth in ground_truths:\n","    f1 = calc_f1(predicted, ground_truth)\n","    max_f1 = max(max_f1, f1)\n","  try:\n","    ground_truths[0]\n","  except Exception as e:\n","    if predicted!=predicted:\n","      max_f1 = 1\n","  return max_f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3AX9C3mcNO6-"},"outputs":[],"source":["# NOT allowed to make changes. \n","\n","# Evaluation methodology.\n","metrics = {}\n","total_f1=0\n","total_f2=0\n","pred = pd.read_csv(result_path)\n","truth = pd.read_csv(ground_truth_path)\n","truth.paragraph_id = truth.paragraph_id\n","# truth.paragraph_id = truth.paragraph_id.apply(literal_eval)\n","# truth.answers = truth.answers.apply(literal_eval)\n","questions = pd.read_csv(ques_data_path)\n","for idx in pred.index:\n","  q_id = pred[\"question_id\"][idx]\n","  q_rows = questions.loc[questions['id'] == q_id].iloc[-1]\n","  theme = q_rows[\"theme\"]\n","  predicted_paragraph = pred[\"paragraph_id\"][idx]\n","  predicted_ans = pred[\"answers\"][idx]\n","  \n","  if theme not in metrics.keys():\n","    metrics[theme] = {\"true_positive\": 0, \"true_negative\": 0, \"total_predictions\": 0, \"f1_sum\": 0}\n","\n","  truth_row = truth.loc[truth['question_id'] == q_id].iloc[-1]\n","  truth_paragraph_id = truth_row[\"paragraph_id\"]\n","  # -1 prediction in case there is no paragraph which can answer the query.\n","  if predicted_paragraph == -1 and truth_row[\"paragraph_id\"] == -1:\n","    # Increase TN.\n","    metrics[theme][\"true_negative\"] = metrics[theme][\"true_negative\"] + 1\n","    total_f2+=1\n","  elif predicted_paragraph == truth_paragraph_id:\n","    # Increase TP for that theme.\n","    metrics[theme][\"true_positive\"] = metrics[theme][\"true_positive\"] + 1\n","    total_f2+=1\n","  # Increase total predictions for that theme.\n","  metrics[theme][\"total_predictions\"] = metrics[theme][\"total_predictions\"] + 1\n","  f1 = calc_f1(predicted_ans, truth_row[\"answers\"])\n","  metrics[theme][\"f1_sum\"] = metrics[theme][\"f1_sum\"] + f1\n","  total_f1+=f1\n","final_f1 = round(total_f1/len(questions),3)\n","final_f2 = round(total_f2/len(questions),3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WyoG7iegNO6_"},"outputs":[],"source":["# NOT allowed to make changes.\n","theme_inf_df = pd.read_csv(cwd+f'/{m}_inf_time.csv')\n","theme_inf_time = {theme:theme_inf_df[theme_inf_df['theme']==theme]['avg_inf_time'].tolist()[0] for theme in metrics}\n","no_of_themes=len(theme_inf_df)\n","# Final score.\n","inf_time_threshold = 1000.0  # milliseconds.\n","final_para_score = 0.0\n","final_qa_score = 0.0\n","\n","for theme in metrics:\n","  inf_time_score = 1.0\n","  metric = metrics[theme]\n","  para_score = (metric[\"true_positive\"] + metric[\"true_negative\"]) / metric[\"total_predictions\"] \n","  qa_score = metric[\"f1_sum\"] / metric[\"total_predictions\"]\n","  avg_inf_time = theme_inf_time[theme] / metric[\"total_predictions\"]\n","  if avg_inf_time > inf_time_threshold:\n","    inf_time_score = inf_time_threshold / avg_inf_time\n","  final_qa_score += 1/(no_of_themes) * inf_time_score * qa_score\n","  final_para_score += 1/(no_of_themes) * inf_time_score * para_score\n","\n","print('f1_score:',final_f1*100)\n","print('para_score:',round(final_para_score,3)*100)\n","print('qa_score:',round(final_qa_score,3)*100)\n","print('average_inference_time:',round(Average(theme_times),0))\n","print('median time:',round(np.sort(theme_times)[len(theme_times)//2],0))\n","print('percent questions cached:',round(num_ques_cache/len(questions),3)*100)"]},{"cell_type":"code","source":[],"metadata":{"id":"mmLK7NNLNO6_"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["c1w0olVIQfVv","iboDsguP61Ov","_CRnwN4e-WF1","534t-fV22mGo","3fjx_8xEuAM3"],"provenance":[],"private_outputs":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}